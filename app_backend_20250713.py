# app_backend_20250713.py ‚Äî backend Flask robuste (RRF/rerank pr√™ts c√¥t√© moteur)

import os
import json
import ctypes
import signal
import psutil
import traceback
from typing import Any, Dict, List, Union

from flask import Flask, request, jsonify
from two_step.auto_restore import record_feedback_event
from two_step.api import semantic_search_phase2_only
from prompt_templates import get_role_prompt
from flask_cors import CORS
from two_step.auto_restore import get_auto_restore_candidate

# --- DLL GTK (logs vus √† l‚Äô√©cran) ---
os.environ["PATH"] = r"C:\GTK3\bin;" + os.environ.get("PATH", "")
for dll in [
    "libgobject-2.0-0.dll", "libglib-2.0-0.dll", "libgio-2.0-0.dll",
    "libpango-1.0-0.dll", "libpangocairo-1.0-0.dll", "libcairo-2.dll",
    "libgdk_pixbuf-2.0-0.dll", "libfontconfig-1.dll",
    "libfreetype-6.dll", "libharfbuzz-0.dll"
]:
    try:
        ctypes.CDLL(os.path.join(r"C:\GTK3\bin", dll))
        print(f"‚úÖ Charg√© depuis backend : {dll}")
    except Exception as e:
        print(f"‚ùå Erreur DLL ({dll}): {e}")




# --- Imports projet ---
from pdf_generator_pdfkit import generate_structured_pdf_from_answer
from structured_output_generator import generate_structured_output_from_answer, group_chunks_by_theme

from feedback_learning_20250713 import save_feedback
from feedback_engine import reload_validated_feedback, load_validated_feedback
from build_feedback_index_by_role import build_feedback_index_for_role

# moteur v1
from semantic_search_20250713 import (
    semantic_search_20250713 as semantic_search,
    load_faiss_indexes,
    hash_question
)



# moteur v2 (two-step, RRF/rerank/boosts c√¥t√© moteur)
from two_step.auto_restore import get_auto_restore_candidate, record_feedback_event, hash_answer, evidence_signature
 

print("üìÑ BACKEND FILE:", os.path.abspath(__file__))


SEMANTIC_INDEX_VERSION = "v2025-07-25+pdf"
ROLES_VALIDES = {"rh", "appel_offres", "general"}
selected_role = None

def _env_bool(name: str, default: bool = False) -> bool:
    val = os.getenv(name)
    if val is None:
        return default
    return str(val).strip().lower() in {"1", "true", "yes", "on"}

DEBUG_RAG = _env_bool("DEBUG_RAG", False)

def check_and_kill_port(port=5050):
    for proc in psutil.process_iter(['pid', 'name']):
        try:
            for conn in proc.connections(kind='inet'):
                if conn.laddr.port == port:
                    print(f"üîï Port {port} occup√© par PID {proc.info['pid']}. Suppression‚Ä¶")
                    os.kill(proc.info['pid'], signal.SIGTERM)
                    return True
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass
    return False

if check_and_kill_port(5050):
    print("‚úÖ Port lib√©r√©")
else:
    print("‚ÑπÔ∏è Port d√©j√† libre")

app = Flask(__name__)
CORS(app)


# --- Health endpoints ---
from datetime import datetime
import platform

STARTED_AT = datetime.utcnow().isoformat() + "Z"

@app.get("/healthz")
def healthz():
    return {
        "ok": True,
        "service": "drive-reader-backend",
        "started_at": STARTED_AT,
        "now": datetime.utcnow().isoformat() + "Z",
        "python": platform.python_version(),
        "pid": os.getpid(),
    }, 200

def _readiness_checks():
    errors = []

    # 1) Import API phase2
    try:
        from two_step.api import semantic_search_phase2_only  # noqa
    except Exception as e:
        errors.append(f"two_step.api import: {e!r}")

    # 2) Dossier feedback accessible
    try:
        os.makedirs("data/feedback", exist_ok=True)
        test_path = os.path.join("data", "feedback", ".healthz.touch")
        open(test_path, "a").close()
        os.remove(test_path)
    except Exception as e:
        errors.append(f"feedback dir: {e!r}")

    return errors

@app.get("/readyz")
def readyz():
    errs = _readiness_checks()
    if errs:
        return {"ok": False, "errors": errs}, 503
    return {"ok": True}, 200





# ---------- Utils backend ----------

def _doc_to_public_dict(doc) -> Dict[str, Any]:
    """Mappe un objet chunk/doc ‚Üí dict JSON-safe pour l‚ÄôUI."""
    meta = getattr(doc, "metadata", {}) or {}
    return {
        "text": getattr(doc, "page_content", ""),
        "score": float(meta.get("total_score", 0.0) or 0.0),
        "source": meta.get("source", ""),
        "entity_score": float(meta.get("entity_match_score", meta.get("entity_score", 0.0)) or 0.0),
        "feedback_penalty": float(meta.get("penalty_feedback", meta.get("feedback_penalty", 0.0)) or 0.0),
        "bonus_metadata": float(meta.get("bonus_metadata", 0.0) or 0.0),
        "faiss_score": float(meta.get("faiss_score", meta.get("score", 0.0)) or 0.0),
    }

def _safe_json_loads(maybe_json: Union[str, Dict, List, None]) -> Union[Dict, List, None]:
    """Tol√®re une sortie texte non strictement JSON (pr√©fix√©e par logs / texte) et renvoie {} si √©chec."""
    if maybe_json is None:
        return None
    if isinstance(maybe_json, (dict, list)):
        return maybe_json
    if not isinstance(maybe_json, str):
        return None
    s = maybe_json.strip()
    if not s:
        return None
    # heuristique: trouver le premier { ou [
    start_brace = s.find("{")
    start_brack = s.find("[")
    starts = [i for i in (start_brace, start_brack) if i >= 0]
    if starts:
        first = min(starts)
        if first > 0:
            s = s[first:]
    try:
        return json.loads(s)
    except Exception as e:
        if DEBUG_RAG:
            print(f"‚ùå _safe_json_loads: {e}")
            print("---- RAW BEGIN ----")
            print(maybe_json[:1000])
            print("---- RAW END ----")
        return {}

def _read_threshold_env() -> float:
    try:
        return float(os.getenv("CHUNK_MIN_SCORE", "0.0"))
    except Exception:
        return 0.0

# ---------- Routes ----------

@app.route("/version", methods=["GET"])
def version():
    return jsonify({
        "status": "ok",
        "version": SEMANTIC_INDEX_VERSION,
        "message": "Backend op√©rationnel"
    })

@app.route("/initialize", methods=["POST"])
def initialize():
    """Enregistre le r√¥le c√¥t√© backend (appel√© par l‚ÄôUI avant /chat[_v2])."""
    global selected_role
    data = request.get_json(silent=True) or {}
    selected_role = data.get("role")
    if not selected_role:
        return jsonify({"error": "Aucun r√¥le fourni"}), 400
    if selected_role not in ROLES_VALIDES:
        return jsonify({"error": f"‚ùå R√¥le invalide : {selected_role}"}), 400
    return jsonify({"message": "R√¥le enregistr√© avec succ√®s"})

@app.route("/check_index", methods=["GET"])
def check_index():
    try:
        vectorstore_docs, vectorstore_context = load_faiss_indexes(role=selected_role)
        if not vectorstore_docs or not vectorstore_context:
            return jsonify({"message": "‚ùå Index introuvable"}), 500
        return jsonify({"message": "‚úÖ Index charg√© avec succ√®s"})
    except Exception as e:
        return jsonify({"message": f"‚ùå Erreur : {str(e)}"}), 500

@app.route("/feedback", methods=["POST"])
def feedback():
    try:
        data = request.get_json()
        save_feedback(data)

        try:
            # Enregistrement dans l‚Äôhistorique d‚Äôauto-restauration
            record_feedback_event(
                question=data.get("question"),
                role=data.get("role"),
                score=int(data.get("score", 0)),
                answer_text=data.get("answer"),
                answer_hash=data.get("answer_hash"),
                evidence=data.get("evidence"),
                sources=data.get("sources", []),
                meta={"mode": data.get("mode")}
            )
        except Exception as e:
            print(f"‚ö†Ô∏è Auto-restore non enregistr√© : {e}")

        return jsonify({"message": "‚úÖ Feedback enregistr√©"})
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e), "traceback": traceback.format_exc()}), 500


# -------- v1: /chat (mono-√©tape historique) --------

@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json(force=True) or {}
        question = (data.get("question") or "").strip()
        role = (data.get("role") or "general").strip()
        top_k = int(data.get("top_k") or 8)

        # --- Fast-path auto-restore : 3√ó5/5 cons√©cutifs + preuves inchang√©es ---
        from two_step.auto_restore import get_auto_restore_candidate  # import local pour √©viter cycles
        cand = None
        try:
            cand = get_auto_restore_candidate(question, role)
        except Exception:
            cand = None

        if cand:
            # les chunks/sources viennent du stockage feedback; on les renvoie tels quels
            return jsonify({
                "answer": cand.get("answer_text", ""),
                "sources": cand.get("sources", []),
                "chunks":  cand.get("chunks", []),
                "debug":   {"auto_restore": True, "count": cand.get("count", 0)},
                "version": SEMANTIC_INDEX_VERSION + "+restored"
            })

        # ---------- Phase 2 seule : recherche sur l‚Äôindex g√©n√©ral ----------
        answer, sources, top_docs, debug = semantic_search_phase2_only(
            query=question, role=role, top_k=top_k
        )


        # ---------- Prompt de r√¥le (m√©tier) ----------
        role_prompt = get_role_prompt(role)

        # ---------- Contexte textuel √† partir des chunks ----------
        def _as_text(d):
            if isinstance(d, dict):
                return d.get("page_content") or d.get("text") or ""
            return getattr(d, "page_content", "") or ""

        context_text = "\n\n".join(_as_text(d) for d in top_docs[:top_k])

        # ---------- Construction du prompt final ----------
        final_prompt = f"""{'{'}role_prompt{'}'}

Question:
{'{'}question{'}'}

Contexte (extraits):
{'{'}context_text{'}'}

Consignes:
- R√©ponds en fran√ßais, de mani√®re concise et sourc√©e si possible.
- Si l'information n'appara√Æt pas dans le contexte, dis-le explicitement.
"""

        # ---------- G√©n√©ration (remplacer par ton appel LLM si n√©cessaire) ----------
        answer_text = ""
        try:
            from generation import generate_text  # optionnel
            answer_text = generate_text(final_prompt)
        except Exception:
            # Fallback extractif pour ne pas planter si le LLM n'est pas branch√©
            answer_text = (answer or "").strip()
            if not answer_text:
                answer_text = "R√©sum√© extractif bas√© sur les meilleurs extraits :\n\n" + context_text[:1800]

        # ---------- Normalisation des chunks en dicts ----------
        

        chunks = [_doc_to_public_dict(d) for d in top_docs]

        return jsonify({
            "answer": answer_text,
            "sources": sources,
            "chunks": chunks,
            "debug": debug,
        })

    except Exception as e:
        import traceback
        return jsonify({"error": str(e), "traceback": traceback.format_exc()}), 500

@app.route('/chat_v2', methods=['POST'])
def chat_v2():
    try:
        data = request.get_json(force=True) or {}
        question = (data.get("question") or "").strip()
        role     = (data.get("role") or "general").strip()
        top_k    = int(data.get("top_k") or 8)

        res = semantic_search_phase2_only(query=question, role=role, top_k=top_k)

        # --- compat tuples / dicts ---
        extra = None
        if isinstance(res, dict):
            answer   = res.get("answer") or res.get("answer_text", "")
            sources  = res.get("sources", [])
            top_docs = res.get("top_docs", res.get("chunks", []))
            debug    = res.get("debug", {})
            # tout le reste dans extra
            extra = {k:v for k,v in res.items() if k not in {"answer","answer_text","sources","top_docs","chunks","debug"}}
        elif isinstance(res, (list, tuple)):
            if len(res) >= 4:
                answer, sources, top_docs, debug, *rest = res
                extra = {"extra": rest} if rest else None
            else:
                # tol√©rance si 3 √©l√©ments (ex: pas de debug)
                if len(res) == 3:
                    answer, sources, top_docs = res
                    debug = {}
                else:
                    raise ValueError(f"Unexpected return arity from semantic_search_phase2_only: {len(res)}")
        else:
            raise TypeError(f"Unexpected return type: {type(res)}")

        # normalisation chunks -> dict pour l‚ÄôUI
        

        chunks = [_doc_to_public_dict(d) for d in (top_docs or [])]

        payload = {
            "answer": answer or "",
            "sources": sources or [],
            "chunks": chunks,
            "debug": debug or {},
        }
        if extra:
            payload["extra"] = extra

        return jsonify(payload)

    except Exception as e:
        import traceback
        return jsonify({"error": str(e), "traceback": traceback.format_exc()}), 500




if __name__ == "__main__":
    import os
    PORT = int(os.getenv("PORT", "5050"))  # ton script parle de 5050
    HOST = os.getenv("HOST", "127.0.0.1")

    # Si tu as une fonction qui ‚Äúlib√®re le port‚Äù (kill du PID), garde-la
    try:
        # free_port_if_needed(PORT)   # <- seulement si tu l‚Äôas vraiment
        pass
    except Exception as e:
        print(f"‚ö†Ô∏è Impossible de lib√©rer le port {PORT}: {e}")

    # LANCE le serveur (c‚Äôest le point manquant)
    app.run(host=HOST, port=PORT)

